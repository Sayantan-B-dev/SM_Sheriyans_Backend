https://github.com/ankurdotio/cohort-project.git


# Comprehensive Guide to Face-api.js with React: From Beginner to Advanced

## üéØ Introduction to Face-api.js

Face-api.js is a JavaScript library built on TensorFlow.js that brings face detection and recognition capabilities directly to the browser. It's perfect for React applications because it runs entirely client-side‚Äîno server processing needed!

### üîß Core Capabilities:
- **Face Detection**: Find faces in images/video
- **Face Landmark Detection**: 68 facial points (eyes, nose, mouth, jaw)
- **Face Recognition**: Identify/verify faces
- **Facial Expression Analysis**: Detect emotions
- **Age & Gender Estimation**
- **Face Alignment**: Normalize face orientation

## üì¶ Project Setup & Initial Configuration

```bash
# Create React app
npx create-react-app face-detection-app
cd face-detection-app

# Install required dependencies
npm install face-api.js react-webcam
npm install @types/face-api.js  # For TypeScript support (optional)
```

## üìÅ File Structure for Our Project

```
src/
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ FaceDetector/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FaceDetector.jsx
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FaceDetector.css
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ useFaceDetection.js  # Custom hook
‚îÇ   ‚îú‚îÄ‚îÄ WebcamCapture/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ WebcamCapture.jsx
‚îÇ   ‚îî‚îÄ‚îÄ ImageUploader/
‚îÇ       ‚îî‚îÄ‚îÄ ImageUploader.jsx
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ faceUtils.js  # Helper functions
‚îú‚îÄ‚îÄ models/           # Downloaded face-api models
‚îú‚îÄ‚îÄ App.jsx
‚îî‚îÄ‚îÄ index.js
```

## üöÄ Step 1: Basic Setup & Model Loading

### **Understanding Models**
Face-api.js uses pre-trained TensorFlow models that MUST be loaded before use:

- **SSD Mobilenet V1**: Face detection (lightweight, fast)
- **Tiny Face Detector**: Even lighter but less accurate
- **Face Landmark 68**: 68-point facial landmarks
- **Face Recognition**: 128-dimension face descriptors
- **Face Expression**: 7 emotions (neutral, happy, sad, angry, fearful, disgusted, surprised)
- **Age & Gender**: Age estimation and gender classification

```javascript
// File: src/utils/faceUtils.js
import * as faceapi from 'face-api.js';

// This utility file handles all face-api operations

/**
 * Loads all necessary face-api models
 * @param {string} modelsPath - Path to models directory
 * @returns {Promise} - Resolves when all models are loaded
 */
export const loadModels = async (modelsPath = '/models') => {
  try {
    console.log('Loading face detection models...');
    
    // Load face detection model (choose one based on your needs)
    await faceapi.nets.ssdMobilenetv1.loadFromUri(modelsPath);
    // Alternative: await faceapi.nets.tinyFaceDetector.loadFromUri(modelsPath);
    
    // Load face landmark model (68 points)
    await faceapi.nets.faceLandmark68Net.loadFromUri(modelsPath);
    
    // Load face recognition model (for face matching)
    await faceapi.nets.faceRecognitionNet.loadFromUri(modelsPath);
    
    // Load facial expression recognition model
    await faceapi.nets.faceExpressionNet.loadFromUri(modelsPath);
    
    // Load age and gender model
    await faceapi.nets.ageGenderNet.loadFromUri(modelsPath);
    
    console.log('All models loaded successfully!');
    return true;
  } catch (error) {
    console.error('Error loading models:', error);
    throw error;
  }
};

/**
 * Detects faces in an image with all features
 * @param {HTMLImageElement | HTMLVideoElement | HTMLCanvasElement} input - Media element
 * @param {Object} options - Detection options
 * @returns {Promise<Array>} - Array of face detections with full details
 */
export const detectFaces = async (input, options = {}) => {
  const detectionOptions = new faceapi.SsdMobilenetv1Options({
    minConfidence: 0.5,          // Minimum confidence threshold (0-1)
    maxResults: 10,              // Maximum number of faces to detect
    ...options                   // Override with custom options
  });

  try {
    // Perform face detection with all features
    const detections = await faceapi
      .detectAllFaces(input, detectionOptions)        // Detect faces
      .withFaceLandmarks()                            // Get 68-point landmarks
      .withFaceDescriptors()                          // Get 128D descriptors for recognition
      .withFaceExpressions()                          // Detect emotions
      .withAgeAndGender();                            // Estimate age and gender

    return detections;
  } catch (error) {
    console.error('Face detection error:', error);
    return [];
  }
};

/**
 * Draws face detections on a canvas
 * @param {HTMLCanvasElement} canvas - Canvas to draw on
 * @param {Array} detections - Face detection results
 * @param {Object} drawOptions - Drawing customization
 */
export const drawDetections = (canvas, detections, drawOptions = {}) => {
  // Clear canvas first
  const ctx = canvas.getContext('2d');
  ctx.clearRect(0, 0, canvas.width, canvas.height);
  
  // Default drawing options
  const options = {
    drawBoxes: true,
    drawLandmarks: true,
    drawExpressions: true,
    drawAgeGender: true,
    lineWidth: 2,
    boxColor: '#00ff00',
    landmarkColor: '#ff0000',
    textColor: '#ffffff',
    textSize: 14,
    ...drawOptions
  };

  // Draw each detection
  detections.forEach(detection => {
    // Draw face bounding box
    if (options.drawBoxes && detection.detection) {
      const box = detection.detection.box;
      drawBox(canvas, box, options);
    }

    // Draw facial landmarks (68 points)
    if (options.drawLandmarks && detection.landmarks) {
      drawLandmarks(canvas, detection.landmarks, options);
    }

    // Draw facial expressions
    if (options.drawExpressions && detection.expressions) {
      drawExpressions(canvas, detection, options);
    }

    // Draw age and gender
    if (options.drawAgeGender && detection.age && detection.gender) {
      drawAgeGender(canvas, detection, options);
    }
  });
};

// Helper drawing functions
const drawBox = (canvas, box, options) => {
  const ctx = canvas.getContext('2d');
  ctx.strokeStyle = options.boxColor;
  ctx.lineWidth = options.lineWidth;
  ctx.strokeRect(box.x, box.y, box.width, box.height);
  
  // Add label
  ctx.fillStyle = options.boxColor;
  ctx.font = `${options.textSize}px Arial`;
  ctx.fillText('Face', box.x, box.y > 10 ? box.y - 5 : 10);
};

const drawLandmarks = (canvas, landmarks, options) => {
  const ctx = canvas.getContext('2d');
  ctx.fillStyle = options.landmarkColor;
  
  // Draw each landmark point
  landmarks.positions.forEach(point => {
    ctx.beginPath();
    ctx.arc(point.x, point.y, 2, 0, 2 * Math.PI);
    ctx.fill();
  });
};

const drawExpressions = (canvas, detection, options) => {
  const ctx = canvas.getContext('2d');
  const box = detection.detection.box;
  
  // Get top expression
  const expressions = detection.expressions;
  const sorted = Object.entries(expressions).sort((a, b) => b[1] - a[1]);
  const topExpression = sorted[0];
  
  // Draw expression text
  ctx.fillStyle = options.textColor;
  ctx.font = `${options.textSize}px Arial`;
  ctx.fillText(
    `${topExpression[0]}: ${(topExpression[1] * 100).toFixed(1)}%`,
    box.x,
    box.y + box.height + 20
  );
};

const drawAgeGender = (canvas, detection, options) => {
  const ctx = canvas.getContext('2d');
  const box = detection.detection.box;
  
  ctx.fillStyle = options.textColor;
  ctx.font = `${options.textSize}px Arial`;
  ctx.fillText(
    `${detection.gender} ${Math.round(detection.age)} years`,
    box.x,
    box.y + box.height + 40
  );
};

/**
 * Calculates similarity between two face descriptors
 * @param {Float32Array} descriptor1 - First face descriptor
 * @param {Float32Array} descriptor2 - Second face descriptor
 * @returns {number} - Distance (lower = more similar)
 */
export const faceDistance = (descriptor1, descriptor2) => {
  return faceapi.euclideanDistance(descriptor1, descriptor2);
};

/**
 * Creates a labeled face descriptor for recognition
 * @param {string} label - Person's name/identifier
 * @param {Float32Array} descriptor - Face descriptor
 * @returns {Object} - Labeled face descriptor
 */
export const createLabeledFaceDescriptor = (label, descriptor) => {
  return new faceapi.LabeledFaceDescriptors(label, [descriptor]);
};

// Export faceapi for direct access if needed
export { faceapi };
```

## üé® Step 2: Creating a Custom React Hook for Face Detection

```javascript
// File: src/hooks/useFaceDetection.js
import { useState, useEffect, useRef, useCallback } from 'react';
import * as faceapi from 'face-api.js';

/**
 * Custom hook for face detection functionality
 * Manages state and provides methods for face operations
 */
const useFaceDetection = (options = {}) => {
  // State declarations using useState hook
  const [isLoading, setIsLoading] = useState(true);
  const [detections, setDetections] = useState([]);
  const [error, setError] = useState(null);
  const [modelsLoaded, setModelsLoaded] = useState(false);
  const [isDetecting, setIsDetecting] = useState(false);
  
  // Refs for canvas and media elements
  const canvasRef = useRef(null);
  const mediaRef = useRef(null);
  
  // Default options
  const defaultOptions = {
    detectionInterval: 100,  // ms between detections
    minConfidence: 0.5,
    modelsPath: '/models',
    drawOptions: {
      drawBoxes: true,
      drawLandmarks: false,
      drawExpressions: true,
      drawAgeGender: true
    },
    ...options
  };

  /**
   * useEffect hook for loading models on component mount
   * Runs once when component is mounted (empty dependency array)
   */
  useEffect(() => {
    loadModels();
    
    // Cleanup function (runs on unmount)
    return () => {
      stopDetection();
    };
  }, []);

  /**
   * Load all face-api models
   */
  const loadModels = async () => {
    try {
      setIsLoading(true);
      
      // Load all required models
      await Promise.all([
        faceapi.nets.ssdMobilenetv1.loadFromUri(defaultOptions.modelsPath),
        faceapi.nets.faceLandmark68Net.loadFromUri(defaultOptions.modelsPath),
        faceapi.nets.faceRecognitionNet.loadFromUri(defaultOptions.modelsPath),
        faceapi.nets.faceExpressionNet.loadFromUri(defaultOptions.modelsPath),
        faceapi.nets.ageGenderNet.loadFromUri(defaultOptions.modelsPath)
      ]);
      
      setModelsLoaded(true);
      setError(null);
    } catch (err) {
      setError(`Failed to load models: ${err.message}`);
      console.error('Model loading error:', err);
    } finally {
      setIsLoading(false);
    }
  };

  /**
   * Detect faces in current media element
   * useCallback memoizes this function to prevent unnecessary re-renders
   */
  const detectFaces = useCallback(async () => {
    if (!modelsLoaded || !mediaRef.current || isDetecting) return;
    
    try {
      setIsDetecting(true);
      
      // Perform detection with all features
      const detections = await faceapi
        .detectAllFaces(
          mediaRef.current,
          new faceapi.SsdMobilenetv1Options({ 
            minConfidence: defaultOptions.minConfidence 
          })
        )
        .withFaceLandmarks()
        .withFaceDescriptors()
        .withFaceExpressions()
        .withAgeAndGender();
      
      setDetections(detections);
      
      // Draw detections if canvas is available
      if (canvasRef.current) {
        drawDetections(canvasRef.current, detections);
      }
      
      return detections;
    } catch (err) {
      setError(`Detection failed: ${err.message}`);
      return [];
    } finally {
      setIsDetecting(false);
    }
  }, [modelsLoaded, defaultOptions.minConfidence, isDetecting]);

  /**
   * Draw detections on canvas
   */
  const drawDetections = (canvas, detections) => {
    if (!canvas || !detections.length) return;
    
    // Match canvas dimensions to media element
    const displaySize = {
      width: mediaRef.current?.videoWidth || mediaRef.current?.width || 640,
      height: mediaRef.current?.videoHeight || mediaRef.current?.height || 480
    };
    
    faceapi.matchDimensions(canvas, displaySize);
    
    // Resize detections to canvas scale
    const resizedDetections = faceapi.resizeResults(detections, displaySize);
    
    // Clear canvas
    const ctx = canvas.getContext('2d');
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    
    // Draw based on options
    if (defaultOptions.drawOptions.drawBoxes) {
      faceapi.draw.drawDetections(canvas, resizedDetections);
    }
    
    if (defaultOptions.drawOptions.drawLandmarks) {
      faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);
    }
    
    if (defaultOptions.drawOptions.drawExpressions) {
      faceapi.draw.drawFaceExpressions(canvas, resizedDetections);
    }
    
    if (defaultOptions.drawOptions.drawAgeGender) {
      resizedDetections.forEach(detection => {
        const { age, gender, genderProbability } = detection;
        const text = `${Math.round(age)} years, ${gender} (${(genderProbability * 100).toFixed(1)}%)`;
        const box = detection.detection.box;
        
        new faceapi.draw.DrawTextField(
          [text],
          { x: box.x, y: box.y + box.height + 20 }
        ).draw(canvas);
      });
    }
  };

  /**
   * Start continuous face detection
   */
  const startDetection = useCallback(() => {
    if (!modelsLoaded || isDetecting) return;
    
    const intervalId = setInterval(async () => {
      await detectFaces();
    }, defaultOptions.detectionInterval);
    
    // Store interval ID for cleanup
    mediaRef.current._detectionInterval = intervalId;
  }, [modelsLoaded, isDetecting, detectFaces, defaultOptions.detectionInterval]);

  /**
   * Stop continuous detection
   */
  const stopDetection = () => {
    if (mediaRef.current?._detectionInterval) {
      clearInterval(mediaRef.current._detectionInterval);
      mediaRef.current._detectionInterval = null;
    }
    setIsDetecting(false);
  };

  /**
   * Take a snapshot from video/media element
   */
  const takeSnapshot = () => {
    if (!mediaRef.current || !canvasRef.current) return null;
    
    const canvas = document.createElement('canvas');
    const context = canvas.getContext('2d');
    
    canvas.width = mediaRef.current.videoWidth || mediaRef.current.width;
    canvas.height = mediaRef.current.videoHeight || mediaRef.current.height;
    
    context.drawImage(mediaRef.current, 0, 0, canvas.width, canvas.height);
    
    return canvas.toDataURL('image/png');
  };

  /**
   * Compare two faces for similarity
   */
  const compareFaces = (descriptor1, descriptor2, threshold = 0.6) => {
    const distance = faceapi.euclideanDistance(descriptor1, descriptor2);
    return {
      distance,
      isMatch: distance < threshold,
      similarity: Math.max(0, 100 - (distance * 100))
    };
  };

  // Return hook API - everything the component will use
  return {
    // State
    isLoading,
    detections,
    error,
    modelsLoaded,
    isDetecting,
    
    // Refs (to be attached to DOM elements)
    canvasRef,
    mediaRef,
    
    // Methods
    detectFaces,
    startDetection,
    stopDetection,
    takeSnapshot,
    compareFaces,
    reloadModels: loadModels,
    
    // Computed values
    hasFaces: detections.length > 0,
    faceCount: detections.length,
    expressions: detections.map(d => d.expressions),
    ages: detections.map(d => d.age),
    genders: detections.map(d => d.gender)
  };
};

export default useFaceDetection;
```

## üì∏ Step 3: Webcam Component with Real-time Detection

```jsx
// File: src/components/WebcamCapture/WebcamCapture.jsx
import React, { useState, useEffect, useCallback } from 'react';
import Webcam from 'react-webcam';
import useFaceDetection from '../../hooks/useFaceDetection';
import './WebcamCapture.css';

/**
 * WebcamCapture Component
 * Demonstrates real-time face detection from webcam feed
 * Shows advanced React patterns: hooks, memoization, effect cleanup
 */
const WebcamCapture = () => {
  // Component state
  const [isCameraActive, setIsCameraActive] = useState(false);
  const [detectionMode, setDetectionMode] = useState('realtime'); // 'realtime' or 'manual'
  const [threshold, setThreshold] = useState(0.5);
  const [snapshots, setSnapshots] = useState([]);
  
  // Custom hook for face detection logic
  const {
    isLoading,
    detections,
    error,
    modelsLoaded,
    isDetecting,
    canvasRef,
    mediaRef,
    detectFaces,
    startDetection,
    stopDetection,
    takeSnapshot,
    faceCount,
    expressions
  } = useFaceDetection({
    detectionInterval: 150,
    minConfidence: threshold,
    drawOptions: {
      drawBoxes: true,
      drawLandmarks: true,
      drawExpressions: true,
      drawAgeGender: true
    }
  });

  /**
   * useEffect for managing detection based on mode
   * Demonstrates conditional effect execution
   */
  useEffect(() => {
    if (detectionMode === 'realtime' && isCameraActive && modelsLoaded) {
      startDetection();
    } else {
      stopDetection();
    }
    
    // Cleanup function - stops detection when component unmounts
    return () => {
      stopDetection();
    };
  }, [detectionMode, isCameraActive, modelsLoaded, startDetection, stopDetection]);

  /**
   * Handle webcam activation
   * useCallback prevents recreation on each render
   */
  const handleStartCamera = useCallback(() => {
    setIsCameraActive(true);
  }, []);

  /**
   * Handle webcam deactivation
   */
  const handleStopCamera = useCallback(() => {
    setIsCameraActive(false);
    stopDetection();
  }, [stopDetection]);

  /**
   * Manual face detection trigger
   */
  const handleManualDetection = useCallback(async () => {
    if (!isCameraActive) return;
    
    const results = await detectFaces();
    console.log('Manual detection results:', results);
  }, [isCameraActive, detectFaces]);

  /**
   * Capture snapshot with face data
   */
  const handleTakeSnapshot = useCallback(() => {
    if (!isCameraActive) return;
    
    const snapshot = takeSnapshot();
    if (snapshot) {
      setSnapshots(prev => [...prev, {
        id: Date.now(),
        image: snapshot,
        timestamp: new Date().toLocaleTimeString(),
        detections: [...detections] // Clone current detections
      }]);
    }
  }, [isCameraActive, takeSnapshot, detections]);

  /**
   * Calculate dominant expression across all faces
   */
  const getDominantExpression = useCallback(() => {
    if (!expressions || !expressions.length) return null;
    
    // Average all expressions across all faces
    const expressionSums = {};
    let faceCount = 0;
    
    expressions.forEach(faceExpressions => {
      if (!faceExpressions) return;
      
      Object.entries(faceExpressions).forEach(([emotion, value]) => {
        expressionSums[emotion] = (expressionSums[emotion] || 0) + value;
      });
      faceCount++;
    });
    
    // Calculate averages
    const expressionAverages = {};
    Object.keys(expressionSums).forEach(emotion => {
      expressionAverages[emotion] = expressionSums[emotion] / faceCount;
    });
    
    // Find dominant emotion
    return Object.entries(expressionAverages)
      .sort((a, b) => b[1] - a[1])[0];
  }, [expressions]);

  // Render loading state
  if (isLoading) {
    return (
      <div className="loading-container">
        <div className="spinner"></div>
        <p>Loading face detection models...</p>
        <small>This may take a few seconds on first load</small>
      </div>
    );
  }

  // Render error state
  if (error) {
    return (
      <div className="error-container">
        <h3>Error Loading Models</h3>
        <p>{error}</p>
        <button onClick={() => window.location.reload()}>Retry</button>
      </div>
    );
  }

  return (
    <div className="webcam-container">
      {/* Header */}
      <div className="header">
        <h2>Real-time Face Detection</h2>
        <div className="stats">
          <span className="stat">Faces: {faceCount}</span>
          <span className="stat">Status: {isDetecting ? 'Detecting' : 'Idle'}</span>
          <span className="stat">Models: {modelsLoaded ? 'Loaded' : 'Loading'}</span>
        </div>
      </div>

      {/* Main content area */}
      <div className="main-content">
        {/* Left panel - Webcam and Canvas */}
        <div className="camera-panel">
          {/* Webcam feed - Hidden but provides video stream to face-api */}
          {isCameraActive && (
            <Webcam
              ref={mediaRef}
              audio={false}
              screenshotFormat="image/jpeg"
              videoConstraints={{
                width: 640,
                height: 480,
                facingMode: "user"
              }}
              style={{ display: 'none' }} // Hidden, used only for processing
            />
          )}

          {/* Canvas overlay for drawing detections */}
          <div className="camera-view">
            {isCameraActive ? (
              <>
                {/* This div acts as our video display */}
                <div className="video-feed">
                  <video
                    ref={el => {
                      // Connect webcam stream to video element
                      if (el && mediaRef.current?.video) {
                        el.srcObject = mediaRef.current.video.srcObject;
                        el.play();
                      }
                    }}
                    autoPlay
                    muted
                    className="video-element"
                  />
                  {/* Canvas overlay for drawing detections */}
                  <canvas
                    ref={canvasRef}
                    className="detection-canvas"
                    width={640}
                    height={480}
                  />
                </div>
                
                {/* Detection information overlay */}
                <div className="detection-overlay">
                  {detections.map((detection, index) => (
                    <div key={index} className="face-info">
                      <div className="face-number">Face #{index + 1}</div>
                      {detection.age && (
                        <div className="face-age">
                          Age: {Math.round(detection.age)} years
                        </div>
                      )}
                      {detection.gender && (
                        <div className="face-gender">
                          Gender: {detection.gender} 
                          ({Math.round(detection.genderProbability * 100)}%)
                        </div>
                      )}
                      {detection.expressions && (
                        <div className="face-expression">
                          Emotion: {
                            Object.entries(detection.expressions)
                              .sort((a, b) => b[1] - a[1])[0][0]
                          }
                        </div>
                      )}
                    </div>
                  ))}
                </div>
              </>
            ) : (
              <div className="camera-placeholder">
                <div className="placeholder-icon">üì∑</div>
                <p>Camera is inactive</p>
              </div>
            )}
          </div>

          {/* Camera controls */}
          <div className="camera-controls">
            {!isCameraActive ? (
              <button
                className="btn btn-primary"
                onClick={handleStartCamera}
                disabled={isLoading}
              >
                Start Camera
              </button>
            ) : (
              <>
                <button
                  className="btn btn-danger"
                  onClick={handleStopCamera}
                >
                  Stop Camera
                </button>
                <button
                  className="btn btn-secondary"
                  onClick={handleManualDetection}
                  disabled={isDetecting}
                >
                  {isDetecting ? 'Detecting...' : 'Detect Faces'}
                </button>
                <button
                  className="btn btn-success"
                  onClick={handleTakeSnapshot}
                  disabled={!faceCount}
                >
                  Take Snapshot
                </button>
              </>
            )}
          </div>
        </div>

        {/* Right panel - Controls and Info */}
        <div className="control-panel">
          {/* Detection mode selector */}
          <div className="control-group">
            <h4>Detection Mode</h4>
            <div className="mode-selector">
              <button
                className={`mode-btn ${detectionMode === 'realtime' ? 'active' : ''}`}
                onClick={() => setDetectionMode('realtime')}
              >
                Realtime
              </button>
              <button
                className={`mode-btn ${detectionMode === 'manual' ? 'active' : ''}`}
                onClick={() => setDetectionMode('manual')}
              >
                Manual
              </button>
            </div>
          </div>

          {/* Confidence threshold slider */}
          <div className="control-group">
            <h4>Detection Sensitivity</h4>
            <div className="slider-container">
              <input
                type="range"
                min="0.1"
                max="0.9"
                step="0.1"
                value={threshold}
                onChange={(e) => setThreshold(parseFloat(e.target.value))}
                className="slider"
              />
              <div className="slider-labels">
                <span>Low ({threshold.toFixed(1)})</span>
                <span>High</span>
              </div>
              <div className="threshold-info">
                Current threshold: {threshold}
                <small>Higher values = fewer but more confident detections</small>
              </div>
            </div>
          </div>

          {/* Statistics panel */}
          <div className="control-group">
            <h4>Detection Statistics</h4>
            <div className="stats-grid">
              <div className="stat-card">
                <div className="stat-value">{faceCount}</div>
                <div className="stat-label">Faces Detected</div>
              </div>
              <div className="stat-card">
                <div className="stat-value">
                  {detections.length > 0 
                    ? Math.round(detections[0].detection.score * 100)
                    : 0}%
                </div>
                <div className="stat-label">Confidence</div>
              </div>
              <div className="stat-card">
                <div className="stat-value">
                  {getDominantExpression()?.[0] || 'N/A'}
                </div>
                <div className="stat-label">Dominant Emotion</div>
              </div>
            </div>
          </div>

          {/* Snapshots gallery */}
          {snapshots.length > 0 && (
            <div className="control-group">
              <h4>Captured Snapshots ({snapshots.length})</h4>
              <div className="snapshots-grid">
                {snapshots.slice(-4).reverse().map(snapshot => (
                  <div key={snapshot.id} className="snapshot-card">
                    <img 
                      src={snapshot.image} 
                      alt={`Snapshot ${snapshot.timestamp}`}
                      className="snapshot-image"
                    />
                    <div className="snapshot-info">
                      <div>{snapshot.timestamp}</div>
                      <div>{snapshot.detections.length} faces</div>
                    </div>
                  </div>
                ))}
              </div>
            </div>
          )}
        </div>
      </div>

      {/* Debug information (collapsible) */}
      <details className="debug-info">
        <summary>Debug Information</summary>
        <pre>
          {JSON.stringify({
            isLoading,
            modelsLoaded,
            isDetecting,
            faceCount,
            detectionMode,
            threshold,
            detections: detections.map(d => ({
              confidence: d.detection.score,
              age: d.age,
              gender: d.gender,
              expressions: d.expressions
            }))
          }, null, 2)}
        </pre>
      </details>
    </div>
  );
};

export default WebcamCapture;
```

## üé≠ Step 4: Advanced Feature - Face Recognition System

```jsx
// File: src/components/FaceRecognition/FaceRecognition.jsx
import React, { useState, useEffect, useRef, useCallback } from 'react';
import * as faceapi from 'face-api.js';
import './FaceRecognition.css';

/**
 * Advanced Face Recognition Component
 * Demonstrates face matching, labeled descriptors, and recognition database
 */
const FaceRecognition = () => {
  // State for recognized faces database
  const [labeledDescriptors, setLabeledDescriptors] = useState([]);
  const [currentLabel, setCurrentLabel] = useState('');
  const [recognitionThreshold, setRecognitionThreshold] = useState(0.6);
  const [matches, setMatches] = useState([]);
  const [isTraining, setIsTraining] = useState(false);
  
  // Refs
  const inputRef = useRef(null);
  const canvasRef = useRef(null);
  const fileInputRef = useRef(null);

  /**
   * Load a known face into the recognition database
   */
  const addFaceToDatabase = useCallback(async (label, imageElement) => {
    if (!label.trim()) {
      alert('Please enter a name for this face');
      return;
    }

    try {
      setIsTraining(true);
      
      // Detect face and get descriptor
      const detection = await faceapi
        .detectSingleFace(imageElement)
        .withFaceLandmarks()
        .withFaceDescriptor();
      
      if (!detection) {
        alert('No face detected in the image');
        return;
      }
      
      // Create or update labeled descriptor
      setLabeledDescriptors(prev => {
        const existingIndex = prev.findIndex(ld => ld.label === label);
        
        if (existingIndex >= 0) {
          // Update existing person with new descriptor
          const updated = [...prev];
          updated[existingIndex].descriptors.push(detection.descriptor);
          return updated;
        } else {
          // Add new person
          return [...prev, {
            label,
            descriptors: [detection.descriptor]
          }];
        }
      });
      
      alert(`Face added to database as "${label}"`);
      setCurrentLabel('');
      
    } catch (error) {
      console.error('Error adding face:', error);
      alert('Failed to add face to database');
    } finally {
      setIsTraining(false);
    }
  }, []);

  /**
   * Recognize faces in an image
   */
  const recognizeFaces = useCallback(async (imageElement) => {
    if (labeledDescriptors.length === 0) {
      alert('Please add some faces to the database first');
      return [];
    }

    try {
      // Detect all faces in the image
      const detections = await faceapi
        .detectAllFaces(imageElement)
        .withFaceLandmarks()
        .withFaceDescriptors();
      
      if (detections.length === 0) {
        return [];
      }
      
      // Create face matcher
      const labeledFaceDescriptors = labeledDescriptors.map(
        ld => new faceapi.LabeledFaceDescriptors(ld.label, ld.descriptors)
      );
      const faceMatcher = new faceapi.FaceMatcher(
        labeledFaceDescriptors,
        recognitionThreshold
      );
      
      // Find matches for each face
      const recognitionResults = detections.map(detection => {
        const match = faceMatcher.findBestMatch(detection.descriptor);
        return {
          detection,
          match,
          label: match.label,
          distance: match.distance,
          isUnknown: match.label === 'unknown'
        };
      });
      
      setMatches(recognitionResults);
      return recognitionResults;
      
    } catch (error) {
      console.error('Recognition error:', error);
      return [];
    }
  }, [labeledDescriptors, recognitionThreshold]);

  /**
   * Handle image upload for training
   */
  const handleImageUpload = useCallback((event) => {
    const file = event.target.files[0];
    if (!file) return;
    
    const reader = new FileReader();
    reader.onload = async (e) => {
      const img = new Image();
      img.onload = () => addFaceToDatabase(currentLabel, img);
      img.src = e.target.result;
    };
    reader.readAsDataURL(file);
    
    // Reset file input
    event.target.value = '';
  }, [currentLabel, addFaceToDatabase]);

  /**
   * Draw recognition results on canvas
   */
  const drawRecognitionResults = useCallback((canvas, image, results) => {
    if (!canvas || !image || !results) return;
    
    // Set canvas dimensions
    canvas.width = image.width;
    canvas.height = image.height;
    
    // Draw image
    const ctx = canvas.getContext('2d');
    ctx.drawImage(image, 0, 0);
    
    // Resize detections to image size
    const resizedResults = faceapi.resizeResults(results.map(r => r.detection), {
      width: image.width,
      height: image.height
    });
    
    // Draw each recognition result
    resizedResults.forEach((detection, i) => {
      const result = results[i];
      const box = detection.detection.box;
      
      // Draw bounding box
      ctx.strokeStyle = result.isUnknown ? '#ff0000' : '#00ff00';
      ctx.lineWidth = 2;
      ctx.strokeRect(box.x, box.y, box.width, box.height);
      
      // Draw label
      ctx.fillStyle = result.isUnknown ? '#ff0000' : '#00ff00';
      ctx.font = '16px Arial';
      
      const labelText = result.isUnknown 
        ? 'Unknown' 
        : `${result.label} (${Math.round((1 - result.distance) * 100)}%)`;
      
      ctx.fillText(labelText, box.x, box.y > 10 ? box.y - 5 : 10);
      
      // Draw landmarks (optional)
      faceapi.draw.drawFaceLandmarks(canvas, detection);
    });
  }, []);

  /**
   * Handle recognition from webcam
   */
  const handleWebcamRecognition = useCallback(async (videoElement) => {
    const results = await recognizeFaces(videoElement);
    
    if (canvasRef.current && videoElement) {
      drawRecognitionResults(canvasRef.current, videoElement, results);
    }
    
    return results;
  }, [recognizeFaces, drawRecognitionResults]);

  return (
    <div className="face-recognition-container">
      <h2>Face Recognition System</h2>
      
      <div className="recognition-layout">
        {/* Left Panel - Database Management */}
        <div className="database-panel">
          <h3>Face Database</h3>
          
          {/* Add new face form */}
          <div className="add-face-form">
            <input
              type="text"
              value={currentLabel}
              onChange={(e) => setCurrentLabel(e.target.value)}
              placeholder="Enter person's name"
              className="name-input"
            />
            <button
              onClick={() => fileInputRef.current?.click()}
              disabled={!currentLabel || isTraining}
              className="upload-btn"
            >
              {isTraining ? 'Training...' : 'Upload Face Image'}
            </button>
            <input
              ref={fileInputRef}
              type="file"
              accept="image/*"
              onChange={handleImageUpload}
              style={{ display: 'none' }}
            />
          </div>
          
          {/* Database entries */}
          <div className="database-entries">
            {labeledDescriptors.length === 0 ? (
              <p className="empty-database">
                No faces in database. Add some faces to get started.
              </p>
            ) : (
              labeledDescriptors.map((entry, index) => (
                <div key={index} className="database-entry">
                  <div className="entry-label">{entry.label}</div>
                  <div className="entry-count">
                    {entry.descriptors.length} sample(s)
                  </div>
                  <button
                    onClick={() => {
                      setLabeledDescriptors(prev => 
                        prev.filter((_, i) => i !== index)
                      );
                    }}
                    className="remove-btn"
                  >
                    Remove
                  </button>
                </div>
              ))
            )}
          </div>
          
          {/* Recognition threshold */}
          <div className="threshold-control">
            <label>Recognition Threshold: {recognitionThreshold.toFixed(2)}</label>
            <input
              type="range"
              min="0.1"
              max="1.0"
              step="0.05"
              value={recognitionThreshold}
              onChange={(e) => setRecognitionThreshold(parseFloat(e.target.value))}
              className="threshold-slider"
            />
            <div className="threshold-help">
              Lower values = more strict matching
            </div>
          </div>
        </div>
        
        {/* Right Panel - Recognition Results */}
        <div className="recognition-panel">
          <h3>Recognition Results</h3>
          
          {/* Image upload for recognition */}
          <div className="recognition-input">
            <input
              ref={inputRef}
              type="file"
              accept="image/*"
              onChange={async (e) => {
                const file = e.target.files[0];
                if (!file) return;
                
                const reader = new FileReader();
                reader.onload = async (event) => {
                  const img = new Image();
                  img.onload = async () => {
                    const results = await recognizeFaces(img);
                    drawRecognitionResults(canvasRef.current, img, results);
                  };
                  img.src = event.target.result;
                };
                reader.readAsDataURL(file);
              }}
              className="file-input"
            />
          </div>
          
          {/* Results canvas */}
          <div className="results-canvas-container">
            <canvas ref={canvasRef} className="results-canvas" />
          </div>
          
          {/* Match list */}
          {matches.length > 0 && (
            <div className="matches-list">
              <h4>Detected Faces:</h4>
              {matches.map((match, index) => (
                <div key={index} className={`match-item ${match.isUnknown ? 'unknown' : 'known'}`}>
                  <div className="match-info">
                    <span className="match-label">
                      {match.isUnknown ? 'Unknown Person' : match.label}
                    </span>
                    <span className="match-confidence">
                      Confidence: {Math.round((1 - match.distance) * 100)}%
                    </span>
                  </div>
                  <div className="match-distance">
                    Distance: {match.distance.toFixed(3)}
                  </div>
                </div>
              ))}
            </div>
          )}
        </div>
      </div>
      
      {/* Statistics */}
      <div className="recognition-stats">
        <div className="stat">
          <div className="stat-value">{labeledDescriptors.length}</div>
          <div className="stat-label">People in Database</div>
        </div>
        <div className="stat">
          <div className="stat-value">
            {labeledDescriptors.reduce((sum, ld) => sum + ld.descriptors.length, 0)}
          </div>
          <div className="stat-label">Total Face Samples</div>
        </div>
        <div className="stat">
          <div className="stat-value">{matches.filter(m => !m.isUnknown).length}</div>
          <div className="stat-label">Recognized Faces</div>
        </div>
      </div>
    </div>
  );
};

export default FaceRecognition;
```

## üéØ Step 5: Main App Component with Routing

```jsx
// File: src/App.jsx
import React, { useState, useEffect } from 'react';
import './App.css';
import { loadModels } from './utils/faceUtils';
import WebcamCapture from './components/WebcamCapture/WebcamCapture';
import FaceRecognition from './components/FaceRecognition/FaceRecognition';
import ImageUploader from './components/ImageUploader/ImageUploader';

/**
 * Main App Component
 * Demonstrates React Router pattern, global state management, and layout
 */
function App() {
  // Global state for the application
  const [modelsLoaded, setModelsLoaded] = useState(false);
  const [loading, setLoading] = useState(true);
  const [activeTab, setActiveTab] = useState('webcam'); // 'webcam', 'recognition', 'upload'
  const [error, setError] = useState(null);

  /**
   * useEffect for initial model loading
   * Runs once when component mounts
   */
  useEffect(() => {
    const initializeFaceAPI = async () => {
      try {
        setLoading(true);
        
        // Load all models at app startup
        await loadModels('/models');
        setModelsLoaded(true);
        setError(null);
        
        console.log('FaceAPI initialized successfully');
      } catch (err) {
        console.error('Failed to initialize FaceAPI:', err);
        setError(`Initialization failed: ${err.message}`);
      } finally {
        setLoading(false);
      }
    };

    initializeFaceAPI();
  }, []);

  // Show loading screen
  if (loading) {
    return (
      <div className="app-loading">
        <div className="loading-content">
          <h1>Face Detection App</h1>
          <div className="loading-spinner"></div>
          <p>Loading AI models...</p>
          <small>This may take a moment on first load</small>
        </div>
      </div>
    );
  }

  // Show error screen
  if (error) {
    return (
      <div className="app-error">
        <h1>‚ùå Initialization Error</h1>
        <p>{error}</p>
        <p>Please ensure:</p>
        <ul>
          <li>You're running on a modern browser</li>
          <li>Models are placed in /public/models folder</li>
          <li>You have internet connection (for CDN models)</li>
        </ul>
        <button onClick={() => window.location.reload()}>Retry</button>
      </div>
    );
  }

  return (
    <div className="app-container">
      {/* Header */}
      <header className="app-header">
        <h1 className="app-title">
          <span className="title-icon">ü§ñ</span>
          Face Detection & Recognition
          <span className="title-icon">üëÅÔ∏è</span>
        </h1>
        <div className="app-status">
          <span className={`status-indicator ${modelsLoaded ? 'loaded' : 'loading'}`}>
            ‚óè
          </span>
          <span className="status-text">
            {modelsLoaded ? 'Models Loaded' : 'Loading Models...'}
          </span>
        </div>
      </header>

      {/* Navigation Tabs */}
      <nav className="app-navigation">
        <button
          className={`nav-button ${activeTab === 'webcam' ? 'active' : ''}`}
          onClick={() => setActiveTab('webcam')}
        >
          <span className="nav-icon">üì∑</span>
          Live Webcam
        </button>
        <button
          className={`nav-button ${activeTab === 'recognition' ? 'active' : ''}`}
          onClick={() => setActiveTab('recognition')}
        >
          <span className="nav-icon">üë§</span>
          Face Recognition
        </button>
        <button
          className={`nav-button ${activeTab === 'upload' ? 'active' : ''}`}
          onClick={() => setActiveTab('upload')}
        >
          <span className="nav-icon">üìÅ</span>
          Image Upload
        </button>
        <button
          className="nav-button"
          onClick={() => {
            if (window.confirm('Reload all AI models?')) {
              window.location.reload();
            }
          }}
        >
          <span className="nav-icon">üîÑ</span>
          Reload Models
        </button>
      </nav>

      {/* Main Content */}
      <main className="app-main">
        {!modelsLoaded ? (
          <div className="models-warning">
            <p>‚ö†Ô∏è Models are still loading. Some features may be unavailable.</p>
          </div>
        ) : (
          <>
            {/* Conditional rendering based on active tab */}
            {activeTab === 'webcam' && <WebcamCapture />}
            {activeTab === 'recognition' && <FaceRecognition />}
            {activeTab === 'upload' && <ImageUploader />}
          </>
        )}
      </main>

      {/* Footer */}
      <footer className="app-footer">
        <div className="footer-info">
          <p>Powered by Face-api.js & TensorFlow.js</p>
          <p className="footer-tech">
            Technologies: React Hooks ‚Ä¢ Custom Hooks ‚Ä¢ Canvas API ‚Ä¢ WebRTC
          </p>
        </div>
        <div className="footer-links">
          <a 
            href="https://github.com/justadudewhohacks/face-api.js" 
            target="_blank" 
            rel="noopener noreferrer"
          >
            Face-api.js Docs
          </a>
          <a 
            href="https://reactjs.org/docs/hooks-intro.html" 
            target="_blank" 
            rel="noopener noreferrer"
          >
            React Hooks Docs
          </a>
        </div>
      </footer>
    </div>
  );
}

export default App;
```

## üìÅ Step 6: CSS Styling for Better UI

```css
/* File: src/App.css */
/* Global styles and theming */

.app-container {
  min-height: 100vh;
  display: flex;
  flex-direction: column;
  background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
  color: white;
}

.app-header {
  padding: 1rem 2rem;
  background: rgba(0, 0, 0, 0.2);
  backdrop-filter: blur(10px);
  border-bottom: 1px solid rgba(255, 255, 255, 0.1);
}

.app-title {
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 1rem;
  margin: 0;
  font-size: 2.5rem;
  background: linear-gradient(45deg, #fff, #a0a0a0);
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
}

.app-navigation {
  display: flex;
  justify-content: center;
  gap: 1rem;
  padding: 1rem;
  background: rgba(0, 0, 0, 0.1);
}

.nav-button {
  padding: 0.75rem 1.5rem;
  background: rgba(255, 255, 255, 0.1);
  border: 1px solid rgba(255, 255, 255, 0.2);
  color: white;
  border-radius: 8px;
  cursor: pointer;
  display: flex;
  align-items: center;
  gap: 0.5rem;
  transition: all 0.3s ease;
}

.nav-button:hover {
  background: rgba(255, 255, 255, 0.2);
  transform: translateY(-2px);
}

.nav-button.active {
  background: rgba(255, 255, 255, 0.3);
  box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
}

.app-main {
  flex: 1;
  padding: 2rem;
  max-width: 1400px;
  margin: 0 auto;
  width: 100%;
}

/* File: src/components/WebcamCapture/WebcamCapture.css */
.webcam-container {
  background: rgba(255, 255, 255, 0.05);
  border-radius: 20px;
  padding: 2rem;
  backdrop-filter: blur(10px);
  border: 1px solid rgba(255, 255, 255, 0.1);
}

.camera-view {
  position: relative;
  width: 640px;
  height: 480px;
  margin: 0 auto;
  border-radius: 10px;
  overflow: hidden;
  box-shadow: 0 20px 40px rgba(0, 0, 0, 0.3);
}

.video-element {
  width: 100%;
  height: 100%;
  object-fit: cover;
}

.detection-canvas {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  pointer-events: none;
}

.detection-overlay {
  position: absolute;
  bottom: 0;
  left: 0;
  right: 0;
  background: rgba(0, 0, 0, 0.7);
  padding: 1rem;
  display: flex;
  gap: 1rem;
  overflow-x: auto;
}

.face-info {
  background: rgba(255, 255, 255, 0.1);
  padding: 0.5rem 1rem;
  border-radius: 5px;
  min-width: 150px;
}

/* File: src/components/FaceRecognition/FaceRecognition.css */
.face-recognition-container {
  background: rgba(255, 255, 255, 0.05);
  border-radius: 20px;
  padding: 2rem;
  backdrop-filter: blur(10px);
}

.recognition-layout {
  display: grid;
  grid-template-columns: 300px 1fr;
  gap: 2rem;
  margin-top: 2rem;
}

.database-panel {
  background: rgba(0, 0, 0, 0.2);
  padding: 1.5rem;
  border-radius: 10px;
}

.results-canvas {
  max-width: 100%;
  border-radius: 10px;
  box-shadow: 0 10px 30px rgba(0, 0, 0, 0.3);
}
```

## üìù Step 7: Important Notes & Best Practices

### **Model Download & Setup**
```bash
# Download required models (place in public/models/)
# You can get them from:
# https://github.com/justadudewhohacks/face-api.js/tree/master/weights

# Required models:
/public/models/
‚îú‚îÄ‚îÄ age_gender_model-shard1
‚îú‚îÄ‚îÄ age_gender_model-weights_manifest.json
‚îú‚îÄ‚îÄ face_expression_model-shard1
‚îú‚îÄ‚îÄ face_expression_model-weights_manifest.json
‚îú‚îÄ‚îÄ face_landmark_68_model-shard1
‚îú‚îÄ‚îÄ face_landmark_68_model-weights_manifest.json
‚îú‚îÄ‚îÄ face_recognition_model-shard1
‚îú‚îÄ‚îÄ face_recognition_model-weights_manifest.json
‚îú‚îÄ‚îÄ ssd_mobilenetv1_model-shard1
‚îî‚îÄ‚îÄ ssd_mobilenetv1_model-weights_manifest.json
```

### **React Hooks Explained in This Implementation:**

1. **useState**: Manages component state (loading, detections, errors)
2. **useEffect**: Handles side effects (loading models, starting/stoping detection)
3. **useRef**: References DOM elements (canvas, video, file inputs)
4. **useCallback**: Memoizes functions to prevent unnecessary re-renders
5. **Custom Hooks**: Encapsulates face detection logic (useFaceDetection)

### **Performance Optimization Tips:**

```javascript
// 1. Throttle detection in real-time mode
const throttledDetection = useCallback(
  throttle(async () => {
    await detectFaces();
  }, 200), // Run max every 200ms
  [detectFaces]
);

// 2. Use web workers for heavy processing
// 3. Implement virtual scrolling for large result sets
// 4. Use React.memo for expensive components
const MemoizedFaceBox = React.memo(({ detection }) => {
  // Component logic
});

// 5. Implement lazy loading for models
const loadModelOnDemand = async (modelName) => {
  if (!loadedModels[modelName]) {
    await faceapi.nets[modelName].loadFromUri('/models');
    loadedModels[modelName] = true;
  }
};
```

### **Common Issues & Solutions:**

1. **Models not loading**: Ensure CORS headers if loading from CDN
2. **Slow performance**: Reduce input resolution or use TinyFaceDetector
3. **Memory leaks**: Always clear intervals and clean up in useEffect return
4. **Camera permissions**: Handle gracefully with user feedback

## üéØ Conclusion & Next Steps

This comprehensive guide covers:

‚úÖ **Basic Setup**: Installing and configuring face-api.js  
‚úÖ **Model Management**: Loading and using different AI models  
‚úÖ **Real-time Detection**: Webcam integration with canvas overlay  
‚úÖ **Face Recognition**: Building a face matching system  
‚úÖ **React Integration**: Custom hooks, state management, performance  
‚úÖ **UI/UX**: Creating an intuitive interface with React  

### **Advanced Topics to Explore:**

1. **Face Filter Effects**: Apply AR filters using landmarks
2. **Face Swapping**: Swap faces between images
3. **Emotion Tracking**: Real-time emotion analytics
4. **Age Progression**: Simulate aging effects
5. **3D Face Reconstruction**: Convert 2D to 3D models
6. **Face Anti-Spoofing**: Detect fake faces/spoofing attempts

### **Production Considerations:**

- Implement error boundaries
- Add loading skeletons
- Optimize bundle size with code splitting
- Add PWA capabilities for offline use
- Implement analytics for detection metrics
- Add accessibility features (screen reader support)

